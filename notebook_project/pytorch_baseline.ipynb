{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import *\n",
    "from transformers.optimization import AdamW\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertwwm_tokenizer =BertTokenizer.from_pretrained('./preTrainModel/chinese-bert-wwm-ext/')\n",
    "device=torch.device(\"cuda\")\n",
    "target_dir='./models/'\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_left = pd.read_csv('./train/train.query.tsv',sep='\\t',header=None)\n",
    "train_left.columns=['id','query']\n",
    "train_right = pd.read_csv('./train/train.reply.tsv',sep='\\t',header=None)\n",
    "train_right.columns=['id','id_sub','reply','label']\n",
    "train_data = train_left.merge(train_right, how='left')\n",
    "train_data['reply'] = train_data['reply'].fillna('好的')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_left = pd.read_csv('./test/test.query.tsv',sep='\\t',header=None, encoding='gbk')\n",
    "test_left.columns = ['id','query']\n",
    "test_right =  pd.read_csv('./test/test.reply.tsv',sep='\\t',header=None, encoding='gbk')\n",
    "test_right.columns=['id','id_sub','reply']\n",
    "df_test = test_left.merge(test_right, how='left')\n",
    "df_test['label']=666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>id_sub</th>\n",
       "      <th>reply</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>采荷一小是分校吧</td>\n",
       "      <td>0</td>\n",
       "      <td>杭州市采荷第一小学钱江苑校区，杭州市钱江新城实验学校。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>采荷一小是分校吧</td>\n",
       "      <td>1</td>\n",
       "      <td>是的</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>采荷一小是分校吧</td>\n",
       "      <td>2</td>\n",
       "      <td>这是5楼</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>毛坯吗？</td>\n",
       "      <td>0</td>\n",
       "      <td>因为公积金贷款贷的少</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>毛坯吗？</td>\n",
       "      <td>1</td>\n",
       "      <td>是呢</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21580</th>\n",
       "      <td>5998</td>\n",
       "      <td>您好，我正在看尚林家园的房子</td>\n",
       "      <td>1</td>\n",
       "      <td>有啊</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21581</th>\n",
       "      <td>5998</td>\n",
       "      <td>您好，我正在看尚林家园的房子</td>\n",
       "      <td>2</td>\n",
       "      <td>我带你看看</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21582</th>\n",
       "      <td>5999</td>\n",
       "      <td>今天可以安排看房子吗？</td>\n",
       "      <td>0</td>\n",
       "      <td>我约下房东，稍后回你</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>5999</td>\n",
       "      <td>今天可以安排看房子吗？</td>\n",
       "      <td>1</td>\n",
       "      <td>可以看，你几点有时间过来呢？</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>5999</td>\n",
       "      <td>今天可以安排看房子吗？</td>\n",
       "      <td>2</td>\n",
       "      <td>好的，那咱们在一号门口这碰头？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21585 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id           query  id_sub                        reply  label\n",
       "0         0        采荷一小是分校吧       0  杭州市采荷第一小学钱江苑校区，杭州市钱江新城实验学校。      1\n",
       "1         0        采荷一小是分校吧       1                           是的      0\n",
       "2         0        采荷一小是分校吧       2                         这是5楼      0\n",
       "3         1            毛坯吗？       0                   因为公积金贷款贷的少      0\n",
       "4         1            毛坯吗？       1                           是呢      0\n",
       "...     ...             ...     ...                          ...    ...\n",
       "21580  5998  您好，我正在看尚林家园的房子       1                           有啊      0\n",
       "21581  5998  您好，我正在看尚林家园的房子       2                        我带你看看      0\n",
       "21582  5999     今天可以安排看房子吗？       0                   我约下房东，稍后回你      1\n",
       "21583  5999     今天可以安排看房子吗？       1               可以看，你几点有时间过来呢？      1\n",
       "21584  5999     今天可以安排看房子吗？       2              好的，那咱们在一号门口这碰头？      0\n",
       "\n",
       "[21585 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>id_sub</th>\n",
       "      <th>reply</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>东区西区？什么时候下证？</td>\n",
       "      <td>0</td>\n",
       "      <td>我在给你发套</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>东区西区？什么时候下证？</td>\n",
       "      <td>1</td>\n",
       "      <td>您看下我发的这几套</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>东区西区？什么时候下证？</td>\n",
       "      <td>2</td>\n",
       "      <td>这两套也是金源花园的</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>东区西区？什么时候下证？</td>\n",
       "      <td>3</td>\n",
       "      <td>价钱低</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>东区西区？什么时候下证？</td>\n",
       "      <td>4</td>\n",
       "      <td>便宜的房子，一般都是顶楼</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53752</th>\n",
       "      <td>13998</td>\n",
       "      <td>这套房子有啥问题吗  我看价格不高</td>\n",
       "      <td>3</td>\n",
       "      <td>租约还有两年</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53753</th>\n",
       "      <td>13998</td>\n",
       "      <td>这套房子有啥问题吗  我看价格不高</td>\n",
       "      <td>4</td>\n",
       "      <td>都有学位的</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53754</th>\n",
       "      <td>13999</td>\n",
       "      <td>我看看时间吧</td>\n",
       "      <td>0</td>\n",
       "      <td>没有呢</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53755</th>\n",
       "      <td>13999</td>\n",
       "      <td>我看看时间吧</td>\n",
       "      <td>1</td>\n",
       "      <td>今天新上的</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53756</th>\n",
       "      <td>13999</td>\n",
       "      <td>我看看时间吧</td>\n",
       "      <td>2</td>\n",
       "      <td>房子我也没看过呢，不知道是几号楼</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53757 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id              query  id_sub             reply  label\n",
       "0          0       东区西区？什么时候下证？       0            我在给你发套    666\n",
       "1          0       东区西区？什么时候下证？       1         您看下我发的这几套    666\n",
       "2          0       东区西区？什么时候下证？       2        这两套也是金源花园的    666\n",
       "3          0       东区西区？什么时候下证？       3               价钱低    666\n",
       "4          0       东区西区？什么时候下证？       4      便宜的房子，一般都是顶楼    666\n",
       "...      ...                ...     ...               ...    ...\n",
       "53752  13998  这套房子有啥问题吗  我看价格不高       3            租约还有两年    666\n",
       "53753  13998  这套房子有啥问题吗  我看价格不高       4             都有学位的    666\n",
       "53754  13999             我看看时间吧       0               没有呢    666\n",
       "53755  13999             我看看时间吧       1             今天新上的    666\n",
       "53756  13999             我看看时间吧       2  房子我也没看过呢，不知道是几号楼    666\n",
       "\n",
       "[53757 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrecessForSentence(Dataset):\n",
    "    \"\"\"\n",
    "    对文本进行处理\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_tokenizer, df, input_categories,max_char_len = 103):\n",
    "        \"\"\"\n",
    "        bert_tokenizer :分词器\n",
    "        file     :语料文件\n",
    "        \"\"\"\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.max_seq_len = max_char_len\n",
    "        self.seqs, self.seq_masks, self.seq_segments, self.labels = self.get_input(df,input_categories, self.bert_tokenizer, self.max_seq_len)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx], self.seq_masks[idx], self.seq_segments[idx], self.labels[idx]\n",
    "    \n",
    "    def _convert_to_transformer_inputs(self,question, answer, tokenizer, max_sequence_length):\n",
    "        def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "            inputs = tokenizer.encode_plus(str1, str2,\n",
    "                add_special_tokens=True,\n",
    "                max_length=length,\n",
    "                truncation_strategy=truncation_strategy,\n",
    "                #truncation=True\n",
    "                )\n",
    "\n",
    "            input_ids =  inputs[\"input_ids\"]\n",
    "            input_masks = [1] * len(input_ids)\n",
    "            input_segments = inputs[\"token_type_ids\"]\n",
    "            padding_length = length - len(input_ids)\n",
    "            padding_id = tokenizer.pad_token_id\n",
    "            input_ids = input_ids + ([padding_id] * padding_length)\n",
    "            input_masks = input_masks + ([0] * padding_length)\n",
    "            input_segments = input_segments + ([0] * padding_length)\n",
    "\n",
    "            return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "        input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "            question, answer, 'longest_first', max_sequence_length)\n",
    "\n",
    "        return [input_ids_q, input_masks_q, input_segments_q]\n",
    "        \n",
    "    # 获取文本与标签\n",
    "    def get_input(self, df,columns, tokenizer, max_sequence_length,test=False):\n",
    "        \"\"\"\n",
    "        通对输入文本进行分词、ID化、截断、填充等流程得到最终的可用于模型输入的序列。\n",
    "        入参:\n",
    "            dataset     : pandas的dataframe格式，包含三列，第一,二列为文本，第三列为标签。标签取值为{0,1}，其中0表示负样本，1代表正样本。\n",
    "            max_seq_len : 目标序列长度，该值需要预先对文本长度进行分别得到，可以设置为小于等于512（BERT的最长文本序列长度为512）的整数。\n",
    "        出参:\n",
    "            seq         : 在入参seq的头尾分别拼接了'CLS'与'SEP'符号，如果长度仍小于max_seq_len，则使用0在尾部进行了填充。\n",
    "            seq_mask    : 只包含0、1且长度等于seq的序列，用于表征seq中的符号是否是有意义的，如果seq序列对应位上为填充符号，\n",
    "                          那么取值为1，否则为0。\n",
    "            seq_segment : shape等于seq，因为是单句，所以取值都为0。\n",
    "            labels      : 标签取值为{0,1}，其中0表示负样本，1代表正样本。\n",
    "        \"\"\"\n",
    "#         df = pd.read_csv(file)\n",
    "#         sentences_1 = map(HanziConv.toSimplified, df['query'].values)\n",
    "#         sentences_2 = map(HanziConv.toSimplified, df['reply'].values)\n",
    "#         labels = df['label'].values\n",
    "#         # 切词\n",
    "#         tokens_seq_1 = list(map(self.bert_tokenizer.tokenize, sentences_1))\n",
    "#         tokens_seq_2 = list(map(self.bert_tokenizer.tokenize, sentences_2))\n",
    "#         # 获取定长序列及其mask\n",
    "#         result = list(map(self.trunate_and_pad, tokens_seq_1, tokens_seq_2))\n",
    "#         seqs = [i[0] for i in result]\n",
    "#         seq_masks = [i[1] for i in result]\n",
    "#         seq_segments = [i[2] for i in result]\n",
    "#         return torch.Tensor(seqs).type(torch.long), torch.Tensor(seq_masks).type(torch.long), torch.Tensor(seq_segments).type(torch.long), torch.Tensor(labels).type(torch.long)\n",
    "        input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "        input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "        for _, instance in tqdm(df[columns].iterrows()):\n",
    "            query,reply = instance.query, instance.reply\n",
    "\n",
    "            ids_q, masks_q, segments_q= \\\n",
    "            self._convert_to_transformer_inputs(query, reply, tokenizer, max_sequence_length)\n",
    "\n",
    "            input_ids_q.append(ids_q)\n",
    "            input_masks_q.append(masks_q)\n",
    "            input_segments_q.append(segments_q)\n",
    "            \n",
    "        labels = df['label'].values\n",
    "        return torch.Tensor(input_ids_q).type(torch.long),torch.Tensor(input_masks_q).type(torch.long),torch.Tensor(input_segments_q).type(torch.long),torch.Tensor(labels).type(torch.long)\n",
    "#         return [np.asarray(input_ids_q, dtype=np.int32), \n",
    "#                 np.asarray(input_masks_q, dtype=np.int32), \n",
    "#                 np.asarray(input_segments_q, dtype=np.int32)]\n",
    "        \n",
    "    \n",
    "#     def trunate_and_pad(self, tokens_seq_1, tokens_seq_2):\n",
    "#         \"\"\"\n",
    "#         1. 如果是单句序列，按照BERT中的序列处理方式，需要在输入序列头尾分别拼接特殊字符'CLS'与'SEP'，\n",
    "#            因此不包含两个特殊字符的序列长度应该小于等于max_seq_len-2，如果序列长度大于该值需要那么进行截断。\n",
    "#         2. 对输入的序列 最终形成['CLS',seq,'SEP']的序列，该序列的长度如果小于max_seq_len，那么使用0进行填充。\n",
    "#         入参: \n",
    "#             seq_1       : 输入序列，在本处其为单个句子。\n",
    "#             seq_2       : 输入序列，在本处其为单个句子。\n",
    "#             max_seq_len : 拼接'CLS'与'SEP'这两个特殊字符后的序列长度\n",
    "        \n",
    "#         出参:\n",
    "#             seq         : 在入参seq的头尾分别拼接了'CLS'与'SEP'符号，如果长度仍小于max_seq_len，则使用0在尾部进行了填充。\n",
    "#             seq_mask    : 只包含0、1且长度等于seq的序列，用于表征seq中的符号是否是有意义的，如果seq序列对应位上为填充符号，\n",
    "#                           那么取值为1，否则为0。\n",
    "#             seq_segment : shape等于seq，单句，取值都为0 ，双句按照01切分\n",
    "           \n",
    "#         \"\"\"\n",
    "#         # 对超长序列进行截断\n",
    "#         if len(tokens_seq_1) > ((self.max_seq_len - 3)//2):\n",
    "#             tokens_seq_1 = tokens_seq_1[0:(self.max_seq_len - 3)//2]\n",
    "#         if len(tokens_seq_2) > ((self.max_seq_len - 3)//2):\n",
    "#             tokens_seq_2 = tokens_seq_2[0:(self.max_seq_len - 3)//2]\n",
    "#         # 分别在首尾拼接特殊符号\n",
    "#         seq = ['[CLS]'] + tokens_seq_1 + ['[SEP]'] + tokens_seq_2 + ['[SEP]']\n",
    "#         seq_segment = [0] * (len(tokens_seq_1) + 2) + [1] * (len(tokens_seq_2) + 1)\n",
    "#         # ID化\n",
    "#         seq = self.bert_tokenizer.convert_tokens_to_ids(seq)\n",
    "#         # 根据max_seq_len与seq的长度产生填充序列\n",
    "#         padding = [0] * (self.max_seq_len - len(seq))\n",
    "#         # 创建seq_mask\n",
    "#         seq_mask = [1] * len(seq) + padding\n",
    "#         # 创建seq_segment\n",
    "#         seq_segment = seq_segment + padding\n",
    "#         # 对seq拼接填充序列\n",
    "#         seq += padding\n",
    "#         assert len(seq) == self.max_seq_len\n",
    "#         assert len(seq_mask) == self.max_seq_len\n",
    "#         assert len(seq_segment) == self.max_seq_len\n",
    "#         return seq, seq_mask, seq_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertwwmModel(nn.Module):\n",
    "    def __init__(self,dropout=0.5,num_classes=2):\n",
    "        super(BertwwmModel,self).__init__()\n",
    "#       self.albert=AlbertForSequenceClassification.from_pretrained('./preTrainModel/albert_chinese_base/')\n",
    "        config = BertConfig.from_pretrained('./preTrainModel/chinese-bert-wwm-ext/') \n",
    "        config.output_hidden_states = False \n",
    "        self.bertwwm = BertModel.from_pretrained('./preTrainModel/chinese-bert-wwm-ext/', \n",
    "                                             config=config)\n",
    "        self.dropout=dropout\n",
    "        self.device=torch.device(\"cuda\")\n",
    "        self.num_classes = num_classes\n",
    "        self.linear = nn.Linear(768*4, num_classes)\n",
    "        for param in self.bertwwm.parameters():\n",
    "            param.requires_grad=True\n",
    "    \n",
    "    def forward(self, q_id, q_mask, q_atn):\n",
    "        q_embedding = self.bertwwm(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "#         q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding) \n",
    "        q = nn.AdaptiveAvgPool2d((1,q_embedding.shape[-1]))(q_embedding).squeeze(1)\n",
    "#         a = tf.keras.layers.GlobalMaxPooling1D()(q_embedding)\n",
    "        a = nn.AdaptiveMaxPool2d((1,q_embedding.shape[-1]))(q_embedding).squeeze(1)\n",
    "        t = q_embedding[:,-1]\n",
    "        e = q_embedding[:, 0]\n",
    "#         x = tf.keras.layers.Concatenate()([q, a,t,e])\n",
    "        merged = torch.cat([q, a, t, e], dim=1)\n",
    "#         print(merged.shape)\n",
    "        x = nn.Dropout(self.dropout)(merged)\n",
    "#         x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "        logits=self.linear(x)\n",
    "#         print(logits.shape)\n",
    "        probabilities =F.softmax(logits, dim=-1)\n",
    "        return logits,probabilities\n",
    "    \n",
    "    \n",
    "# # -------------------- Model definition ------------------- #\n",
    "# print(\"\\t* Building model...\")\n",
    "# model = AlbertModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# PATH='./'\n",
    "# train_file = PATH+'train.csv'\n",
    "# dev_file = PATH+'dev.csv'\n",
    "# test_file=PATH+'test.csv'\n",
    "# test_file = PATH+'test.csv'\n",
    "# bert_tokenizer, df, input_categories,max_char_len = 103\n",
    "# input_categories = ['query','reply']\n",
    "# output_categories = 'label'\n",
    "# MAX_SEQUENCE_LENGTH = 100\n",
    "# print(\"\\t* Loading training data...\")\n",
    "# df_train = pd.read_csv(train_file)\n",
    "# df_dev = pd.read_csv(dev_file)\n",
    "# inputs = DataPrecessForSentence(albert_tokenizer, train_data,input_categories,MAX_SEQUENCE_LENGTH)\n",
    "# train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "# print(\"\\t* Loading validation data...\")\n",
    "# dev_data = DataPrecessForSentence(albert_tokenizer,df_dev,input_categories,MAX_SEQUENCE_LENGTH)\n",
    "# dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------- Model definition ------------------- #\n",
    "# print(\"\\t* Building model...\")\n",
    "# model = AlbertModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_predictions(output_probabilities, targets):\n",
    "\n",
    "    _, out_classes = output_probabilities.max(dim=1)\n",
    "    correct = (out_classes == targets).sum()\n",
    "    return correct.item()\n",
    "\n",
    "\n",
    "def train(model, dataloader,optimizer, criterion,epoch_number, max_gradient_norm):\n",
    "\n",
    "    # Switch the model to train mode.\n",
    "    model.train()\n",
    "    device = model.device\n",
    "    epoch_start = time.time()\n",
    "    batch_time_avg = 0.0\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    tqdm_batch_iterator = tqdm(dataloader)\n",
    "    for batch_index, (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in enumerate(tqdm_batch_iterator):\n",
    "        batch_start = time.time()\n",
    "        # Move input and output data to the GPU if it is used.\n",
    "        seqs, masks, segments, labels = batch_seqs.to(device), batch_seq_masks.to(device), batch_seq_segments.to(device), batch_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, probs  = model(seqs, masks, segments)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n",
    "        optimizer.step()\n",
    "        batch_time_avg += time.time() - batch_start\n",
    "        running_loss += loss.item()\n",
    "        correct_preds += correct_predictions(probs, labels)\n",
    "        description = \"Avg. batch proc. time: {:.4f}s, loss: {:.4f}\"\\\n",
    "                      .format(batch_time_avg/(batch_index+1), running_loss/(batch_index+1))\n",
    "        tqdm_batch_iterator.set_description(description)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = correct_preds / len(dataloader.dataset)\n",
    "    return epoch_time, epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion):\n",
    "\n",
    "    # Switch to evaluate mode.\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    epoch_start = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    all_prob = []\n",
    "    all_labels = []\n",
    "    # Deactivate autograd for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in dataloader:\n",
    "            # Move input and output data to the GPU if one is used.\n",
    "            seqs = batch_seqs.to(device)\n",
    "            masks = batch_seq_masks.to(device)\n",
    "            segments = batch_seq_segments.to(device)\n",
    "            labels = batch_labels.to(device)\n",
    "            logits, probs = model(seqs, masks, segments)\n",
    "            loss = criterion(logits, labels)\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += correct_predictions(probs, labels)\n",
    "            all_prob.extend(probs[:,1].cpu().numpy())\n",
    "            all_labels.extend(batch_labels)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = running_accuracy / (len(dataloader.dataset))\n",
    "    return epoch_time, epoch_loss, epoch_accuracy, roc_auc_score(all_labels, all_prob)\n",
    "\n",
    "\n",
    "\n",
    "def test(model, dataloader):\n",
    "    # Switch the model to eval mode.\n",
    "    label_res=[]\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    time_start = time.time()\n",
    "    batch_time = 0.0\n",
    "    \n",
    "    # Deactivate autograd for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in dataloader:\n",
    "            batch_start = time.time()\n",
    "            # Move input and output data to the GPU if one is used.\n",
    "            seqs, masks, segments, labels = batch_seqs.to(device), batch_seq_masks.to(device), batch_seq_segments.to(device), batch_labels.to(device)\n",
    "            _, probabilities = model(seqs, masks, segments)\n",
    "            _, out_classes = probabilities.max(dim=1)\n",
    "#             print(out_classes)\n",
    "            label_res.extend(out_classes.cpu().numpy())\n",
    "            batch_time += time.time() - batch_start\n",
    "\n",
    "    batch_time /= len(dataloader)\n",
    "    total_time = time.time() - time_start\n",
    "#     accuracy /= (len(dataloader.dataset))\n",
    "    return batch_time, total_time,label_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score = 0.0\n",
    "# start_epoch = 1\n",
    "# # Data for loss curves plot\n",
    "# epochs_count = []\n",
    "# train_losses = []\n",
    "# valid_losses = []\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# _, valid_loss, valid_accuracy, auc = validate(model, dev_loader,criterion)\n",
    "# print(\"\\t* Validation loss before training: {:.4f}, accuracy: {:.4f}%, auc: {:.4f}\".format(valid_loss, (valid_accuracy*100), auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------- Training epochs ------------------- #\n",
    "# print(\"\\n\", 20 * \"=\", \"Training Albert model on device: {}\".format(device), 20 * \"=\")\n",
    "# patience_counter = 0\n",
    "# for epoch in range(start_epoch, epochs + 1):\n",
    "#     epochs_count.append(epoch)\n",
    "#     print(\"* Training epoch {}:\".format(epoch))\n",
    "#     epoch_time, epoch_loss, epoch_accuracy = train(model, train_loader, optimizer, criterion, epoch, max_grad_norm)\n",
    "#     train_losses.append(epoch_loss)\n",
    "#     print(\"-> Training time: {:.4f}s, loss = {:.4f}, accuracy: {:.4f}%\"\n",
    "#           .format(epoch_time, epoch_loss, (epoch_accuracy*100)))\n",
    "#     print(\"* Validation for epoch {}:\".format(epoch))\n",
    "#     epoch_time, epoch_loss, epoch_accuracy , epoch_auc= validate(model, dev_loader,criterion)\n",
    "#     valid_losses.append(epoch_loss)\n",
    "#     print(\"-> Valid. time: {:.4f}s, loss: {:.4f}, accuracy: {:.4f}%, auc: {:.4f}\\n\"\n",
    "#           .format(epoch_time, epoch_loss, (epoch_accuracy*100), epoch_auc))\n",
    "#     # Update the optimizer's learning rate with the scheduler.\n",
    "#     scheduler.step(epoch_accuracy)\n",
    "#     # Early stopping on validation accuracy.\n",
    "#     if epoch_accuracy < best_score:\n",
    "#         patience_counter += 1\n",
    "#     else:\n",
    "#         best_score = epoch_accuracy\n",
    "#         patience_counter = 0\n",
    "#         torch.save({\"epoch\": epoch, \n",
    "#                     \"model\": model.state_dict(),\n",
    "#                     \"best_score\": best_score,\n",
    "#                     \"epochs_count\": epochs_count,\n",
    "#                     \"train_losses\": train_losses,\n",
    "#                     \"valid_losses\": valid_losses},\n",
    "#                     os.path.join(target_dir, \"pytorch_albert_best.pth.tar\"))\n",
    "#     if patience_counter >= patience:\n",
    "#         print(\"-> Early stopping: patience limit reached, stopping...\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_res=[]\n",
    "# def test(model, dataloader):\n",
    "#     # Switch the model to eval mode.\n",
    "#     model.eval()\n",
    "#     device = model.device\n",
    "#     time_start = time.time()\n",
    "#     batch_time = 0.0\n",
    "    \n",
    "#     # Deactivate autograd for evaluation.\n",
    "#     with torch.no_grad():\n",
    "#         for (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in dataloader:\n",
    "#             batch_start = time.time()\n",
    "#             # Move input and output data to the GPU if one is used.\n",
    "#             seqs, masks, segments, labels = batch_seqs.to(device), batch_seq_masks.to(device), batch_seq_segments.to(device), batch_labels.to(device)\n",
    "#             _, probabilities = model(seqs, masks, segments)\n",
    "#             _, out_classes = probabilities.max(dim=1)\n",
    "# #             print(out_classes)\n",
    "#             label_res.extend(out_classes.cpu().numpy())\n",
    "#             batch_time += time.time() - batch_start\n",
    "\n",
    "#     batch_time /= len(dataloader)\n",
    "#     total_time = time.time() - time_start\n",
    "# #     accuracy /= (len(dataloader.dataset))\n",
    "#     return batch_time, total_time,label_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169it [00:00, 1684.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Loading test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53757it [00:33, 1625.25it/s]\n"
     ]
    }
   ],
   "source": [
    "input_categories = ['query','reply']\n",
    "output_categories = 'label'\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "batch_size=128\n",
    "print(\"\\t* Loading test data...\")\n",
    "test_data = DataPrecessForSentence(bertwwm_tokenizer,df_test,input_categories,MAX_SEQUENCE_LENGTH)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time, total_time, label_res= test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17268it [00:10, 1644.17it/s]\n",
      "4317it [00:02, 1644.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model:0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Training Albert model on device: cuda,fold:0 ====================\n",
      "* Training epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 0.7735s, loss: 0.3836: 100%|██████████| 270/270 [03:30<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 210.4318s, loss = 0.3836, accuracy: 83.4260%\n",
      "* Validation for epoch 1:\n",
      "-> Valid. time: 19.2867s, loss: 0.2887, accuracy: 87.3987%, auc: 0.9278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 0.8496s, loss: 0.2562: 100%|██████████| 270/270 [03:51<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 231.8562s, loss = 0.2562, accuracy: 89.6630%\n",
      "* Validation for epoch 2:\n",
      "-> Valid. time: 20.1672s, loss: 0.2746, accuracy: 88.3252%, auc: 0.9378\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 0.8544s, loss: 0.1885: 100%|██████████| 270/270 [03:53<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 233.5977s, loss = 0.1885, accuracy: 92.7322%\n",
      "* Validation for epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170it [00:00, 1697.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Valid. time: 19.6511s, loss: 0.3015, accuracy: 88.0704%, auc: 0.9372\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17268it [00:15, 1105.55it/s]\n",
      "4317it [00:04, 1033.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model:1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Training Albert model on device: cuda,fold:1 ====================\n",
      "* Training epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 0.8478s, loss: 0.3625: 100%|██████████| 270/270 [03:52<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 232.0757s, loss = 0.3625, accuracy: 84.7985%\n",
      "* Validation for epoch 1:\n",
      "-> Valid. time: 19.7465s, loss: 0.3013, accuracy: 87.3292%, auc: 0.9274\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 0.8782s, loss: 0.2460: 100%|██████████| 270/270 [04:00<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 240.9432s, loss = 0.2460, accuracy: 90.0162%\n",
      "* Validation for epoch 2:\n",
      "-> Valid. time: 18.5847s, loss: 0.2889, accuracy: 88.0704%, auc: 0.9349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 0.8547s, loss: 0.1764: 100%|██████████| 270/270 [03:53<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 233.8138s, loss = 0.1764, accuracy: 93.3055%\n",
      "* Validation for epoch 3:\n",
      "-> Valid. time: 18.3110s, loss: 0.3204, accuracy: 88.2789%, auc: 0.9330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17268it [00:14, 1156.15it/s]\n",
      "4317it [00:03, 1247.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model:2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Training Albert model on device: cuda,fold:2 ====================\n",
      "* Training epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 0.8184s, loss: 0.3596: 100%|██████████| 270/270 [03:44<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 224.0291s, loss = 0.3596, accuracy: 84.4394%\n",
      "* Validation for epoch 1:\n",
      "-> Valid. time: 17.8186s, loss: 0.2880, accuracy: 88.0009%, auc: 0.9323\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 0.8424s, loss: 0.2369: 100%|██████████| 270/270 [03:50<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 230.8411s, loss = 0.2369, accuracy: 90.3753%\n",
      "* Validation for epoch 2:\n",
      "-> Valid. time: 18.5753s, loss: 0.2869, accuracy: 88.1399%, auc: 0.9357\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 1.0776s, loss: 0.1772: 100%|██████████| 270/270 [04:55<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 295.4238s, loss = 0.1772, accuracy: 93.0276%\n",
      "* Validation for epoch 3:\n",
      "-> Valid. time: 29.2646s, loss: 0.3021, accuracy: 88.2789%, auc: 0.9379\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17268it [00:47, 361.83it/s] \n",
      "4317it [00:08, 529.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model:3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Training Albert model on device: cuda,fold:3 ====================\n",
      "* Training epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 1.3639s, loss: 0.3470: 100%|██████████| 270/270 [06:18<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 378.1935s, loss = 0.3470, accuracy: 84.9954%\n",
      "* Validation for epoch 1:\n",
      "-> Valid. time: 27.0962s, loss: 0.2658, accuracy: 89.3213%, auc: 0.9388\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 1.2039s, loss: 0.2296: 100%|██████████| 270/270 [05:34<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 334.4795s, loss = 0.2296, accuracy: 90.6532%\n",
      "* Validation for epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Valid. time: 22.7884s, loss: 0.2751, accuracy: 89.2981%, auc: 0.9395\n",
      "\n",
      "* Training epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 1.2602s, loss: 0.1628: 100%|██████████| 270/270 [05:47<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 347.4081s, loss = 0.1628, accuracy: 93.5951%\n",
      "* Validation for epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 69.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Valid. time: 29.0279s, loss: 0.3085, accuracy: 88.6959%, auc: 0.9400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17268it [01:07, 254.24it/s] \n",
      "4317it [00:11, 373.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model:4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Training Albert model on device: cuda,fold:4 ====================\n",
      "* Training epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 1.2491s, loss: 0.3427: 100%|██████████| 270/270 [05:44<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 344.1886s, loss = 0.3427, accuracy: 85.1228%\n",
      "* Validation for epoch 1:\n",
      "-> Valid. time: 27.3842s, loss: 0.2988, accuracy: 87.0512%, auc: 0.9288\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 1.2000s, loss: 0.2356: 100%|██████████| 270/270 [05:32<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 332.3501s, loss = 0.2356, accuracy: 90.3811%\n",
      "* Validation for epoch 2:\n",
      "-> Valid. time: 27.5362s, loss: 0.2956, accuracy: 88.2557%, auc: 0.9344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 1.3358s, loss: 0.1701: 100%|██████████| 270/270 [06:09<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 369.2492s, loss = 0.1701, accuracy: 93.3055%\n",
      "* Validation for epoch 3:\n",
      "-> Valid. time: 29.2513s, loss: 0.3118, accuracy: 88.3484%, auc: 0.9336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input_categories = ['query','reply']\n",
    "# output_categories = 'label'\n",
    "# MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "#N折交叉验证\n",
    "\n",
    "gkf = GroupKFold(n_splits=5).split(X=train_data.reply, groups=train_data.id)\n",
    "\n",
    "valid_preds = [0,0,0,0,0]\n",
    "test_preds = [0,0,0,0,0]\n",
    "\n",
    "batch_size=64\n",
    "epochs=3\n",
    "lr=2e-05\n",
    "patience=3\n",
    "max_grad_norm=10.0\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion = FocalLoss(gamma=0)\n",
    "# label_res=[]\n",
    "oof = np.zeros((len(train_data),1))\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    dev_res=[]\n",
    "    test_res=[]\n",
    "    best_score = 0.0\n",
    "    start_epoch = 1\n",
    "    # Data for loss curves plot\n",
    "    epochs_count = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    #训练集\n",
    "    t_data = DataPrecessForSentence(bertwwm_tokenizer, train_data.iloc[train_idx],input_categories,MAX_SEQUENCE_LENGTH)\n",
    "    train_loader = DataLoader(t_data, shuffle=True, batch_size=batch_size)\n",
    "    #验证集\n",
    "    d_data = DataPrecessForSentence(bertwwm_tokenizer, train_data.iloc[valid_idx],input_categories,MAX_SEQUENCE_LENGTH)\n",
    "    dev_loader = DataLoader(d_data, shuffle=True, batch_size=batch_size)\n",
    "    #开始训练\n",
    "    # -------------------- Model definition ------------------- #\n",
    "    print(\"\\t* Building model:{}...\".format(fold))\n",
    "    model = BertwwmModel().to(device)\n",
    "    \n",
    "    # 待优化的参数\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "            {\n",
    "                    'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                    'weight_decay':0.01\n",
    "            },\n",
    "            {\n",
    "                    'params':[p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                    'weight_decay':0.0\n",
    "            }\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", \n",
    "                                                               factor=0.85, patience=0)\n",
    "    \n",
    "    print(\"\\n\", 20 * \"=\", \"Training Albert model on device: {},fold:{}\".format(device,fold), 20 * \"=\")\n",
    "    patience_counter = 0\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        epochs_count.append(epoch)\n",
    "        print(\"* Training epoch {}:\".format(epoch))\n",
    "        epoch_time, epoch_loss, epoch_accuracy = train(model, train_loader, optimizer, criterion, epoch, max_grad_norm)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(\"-> Training time: {:.4f}s, loss = {:.4f}, accuracy: {:.4f}%\"\n",
    "              .format(epoch_time, epoch_loss, (epoch_accuracy*100)))\n",
    "        print(\"* Validation for epoch {}:\".format(epoch))\n",
    "        epoch_time, epoch_loss, epoch_accuracy , epoch_auc= validate(model, dev_loader,criterion)\n",
    "        valid_losses.append(epoch_loss)\n",
    "        print(\"-> Valid. time: {:.4f}s, loss: {:.4f}, accuracy: {:.4f}%, auc: {:.4f}\\n\"\n",
    "              .format(epoch_time, epoch_loss, (epoch_accuracy*100), epoch_auc))\n",
    "        # Update the optimizer's learning rate with the scheduler.\n",
    "        scheduler.step(epoch_accuracy)\n",
    "        # Early stopping on validation accuracy.\n",
    "        if epoch_accuracy < best_score:\n",
    "            patience_counter += 1 \n",
    "        else:\n",
    "            best_score = epoch_accuracy\n",
    "            patience_counter = 0\n",
    "            \n",
    "            batch_time, total_time, dev_res= test(model, dev_loader)\n",
    "            oof[valid_idx] =[[i] for i in dev_res]\n",
    "            valid_preds[fold]=dev_res\n",
    "            batch_time, total_time, test_res=test(model, test_loader)\n",
    "            test_preds[fold]=test_res\n",
    "#             f1,t = search_f1(valid_outputs, valid_preds[-1])\n",
    "#             print('validation score = ', f1)\n",
    "#             torch.save({\"epoch\": epoch, \n",
    "#                         \"model\": model.state_dict(),\n",
    "#                         \"best_score\": best_score,\n",
    "#                         \"epochs_count\": epochs_count,\n",
    "#                         \"train_losses\": train_losses,\n",
    "#                         \"valid_losses\": valid_losses},\n",
    "#                         os.path.join(target_dir, \"pytorch_albert_best.pth.tar\"))\n",
    "        if patience_counter >= patience:\n",
    "            print(\"-> Early stopping: patience limit reached, stopping...\")\n",
    "            break\n",
    "        \n",
    "#     train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "#     train_outputs = outputs[train_idx]\n",
    "#     valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "#     valid_outputs = outputs[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def search_f1(y_true, y_pred):\n",
    "    best = 0\n",
    "    best_t = 0\n",
    "    for i in range(30,60):\n",
    "        tres = i / 100\n",
    "        y_pred_bin =  (y_pred > tres).astype(int)\n",
    "        score = f1_score(y_true, y_pred_bin)\n",
    "        if score > best:\n",
    "            best = score\n",
    "            best_t = tres\n",
    "    print('best', best)\n",
    "    print('thres', best_t)\n",
    "    return best, best_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = compute_output_arrays(train_data, output_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best 0.25062286610685613\n",
      "thres 0.3\n"
     ]
    }
   ],
   "source": [
    "best_score, best_t = search_f1(outputs,oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = np.average(test_preds, axis=0)\n",
    "sub = sub > best_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['label'] = sub.astype(int)\n",
    "df_test[['id','id_sub','label']].to_csv('./submission_file/submission_bert_wwm.csv',index=False, header=None,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21585"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21585"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53757"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
