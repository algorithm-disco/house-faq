{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import *\n",
    "from transformers.optimization import AdamW\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertwwm_tokenizer =BertTokenizer.from_pretrained('./preTrainModel/chinese-roberta-wwm-ext-large/')\n",
    "device=torch.device(\"cuda\")\n",
    "target_dir='./models/'\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_left = pd.read_csv('./train/train.query.tsv',sep='\\t',header=None)\n",
    "train_left.columns=['id','query']\n",
    "train_right = pd.read_csv('./train/train.reply.tsv',sep='\\t',header=None)\n",
    "train_right.columns=['id','id_sub','reply','label']\n",
    "train_data = train_left.merge(train_right, how='left')\n",
    "train_data['reply'] = train_data['reply'].fillna('好的')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_left = pd.read_csv('./test/test.query.tsv',sep='\\t',header=None, encoding='gbk')\n",
    "test_left.columns = ['id','query']\n",
    "test_right =  pd.read_csv('./test/test.reply.tsv',sep='\\t',header=None, encoding='gbk')\n",
    "test_right.columns=['id','id_sub','reply']\n",
    "df_test = test_left.merge(test_right, how='left')\n",
    "df_test['label']=666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>id_sub</th>\n",
       "      <th>reply</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>采荷一小是分校吧</td>\n",
       "      <td>0</td>\n",
       "      <td>杭州市采荷第一小学钱江苑校区，杭州市钱江新城实验学校。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>采荷一小是分校吧</td>\n",
       "      <td>1</td>\n",
       "      <td>是的</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>采荷一小是分校吧</td>\n",
       "      <td>2</td>\n",
       "      <td>这是5楼</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>毛坯吗？</td>\n",
       "      <td>0</td>\n",
       "      <td>因为公积金贷款贷的少</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>毛坯吗？</td>\n",
       "      <td>1</td>\n",
       "      <td>是呢</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21580</th>\n",
       "      <td>5998</td>\n",
       "      <td>您好，我正在看尚林家园的房子</td>\n",
       "      <td>1</td>\n",
       "      <td>有啊</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21581</th>\n",
       "      <td>5998</td>\n",
       "      <td>您好，我正在看尚林家园的房子</td>\n",
       "      <td>2</td>\n",
       "      <td>我带你看看</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21582</th>\n",
       "      <td>5999</td>\n",
       "      <td>今天可以安排看房子吗？</td>\n",
       "      <td>0</td>\n",
       "      <td>我约下房东，稍后回你</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>5999</td>\n",
       "      <td>今天可以安排看房子吗？</td>\n",
       "      <td>1</td>\n",
       "      <td>可以看，你几点有时间过来呢？</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>5999</td>\n",
       "      <td>今天可以安排看房子吗？</td>\n",
       "      <td>2</td>\n",
       "      <td>好的，那咱们在一号门口这碰头？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21585 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id           query  id_sub                        reply  label\n",
       "0         0        采荷一小是分校吧       0  杭州市采荷第一小学钱江苑校区，杭州市钱江新城实验学校。      1\n",
       "1         0        采荷一小是分校吧       1                           是的      0\n",
       "2         0        采荷一小是分校吧       2                         这是5楼      0\n",
       "3         1            毛坯吗？       0                   因为公积金贷款贷的少      0\n",
       "4         1            毛坯吗？       1                           是呢      0\n",
       "...     ...             ...     ...                          ...    ...\n",
       "21580  5998  您好，我正在看尚林家园的房子       1                           有啊      0\n",
       "21581  5998  您好，我正在看尚林家园的房子       2                        我带你看看      0\n",
       "21582  5999     今天可以安排看房子吗？       0                   我约下房东，稍后回你      1\n",
       "21583  5999     今天可以安排看房子吗？       1               可以看，你几点有时间过来呢？      1\n",
       "21584  5999     今天可以安排看房子吗？       2              好的，那咱们在一号门口这碰头？      0\n",
       "\n",
       "[21585 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>id_sub</th>\n",
       "      <th>reply</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>东区西区？什么时候下证？</td>\n",
       "      <td>0</td>\n",
       "      <td>我在给你发套</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>东区西区？什么时候下证？</td>\n",
       "      <td>1</td>\n",
       "      <td>您看下我发的这几套</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>东区西区？什么时候下证？</td>\n",
       "      <td>2</td>\n",
       "      <td>这两套也是金源花园的</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>东区西区？什么时候下证？</td>\n",
       "      <td>3</td>\n",
       "      <td>价钱低</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>东区西区？什么时候下证？</td>\n",
       "      <td>4</td>\n",
       "      <td>便宜的房子，一般都是顶楼</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53752</th>\n",
       "      <td>13998</td>\n",
       "      <td>这套房子有啥问题吗  我看价格不高</td>\n",
       "      <td>3</td>\n",
       "      <td>租约还有两年</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53753</th>\n",
       "      <td>13998</td>\n",
       "      <td>这套房子有啥问题吗  我看价格不高</td>\n",
       "      <td>4</td>\n",
       "      <td>都有学位的</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53754</th>\n",
       "      <td>13999</td>\n",
       "      <td>我看看时间吧</td>\n",
       "      <td>0</td>\n",
       "      <td>没有呢</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53755</th>\n",
       "      <td>13999</td>\n",
       "      <td>我看看时间吧</td>\n",
       "      <td>1</td>\n",
       "      <td>今天新上的</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53756</th>\n",
       "      <td>13999</td>\n",
       "      <td>我看看时间吧</td>\n",
       "      <td>2</td>\n",
       "      <td>房子我也没看过呢，不知道是几号楼</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53757 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id              query  id_sub             reply  label\n",
       "0          0       东区西区？什么时候下证？       0            我在给你发套    666\n",
       "1          0       东区西区？什么时候下证？       1         您看下我发的这几套    666\n",
       "2          0       东区西区？什么时候下证？       2        这两套也是金源花园的    666\n",
       "3          0       东区西区？什么时候下证？       3               价钱低    666\n",
       "4          0       东区西区？什么时候下证？       4      便宜的房子，一般都是顶楼    666\n",
       "...      ...                ...     ...               ...    ...\n",
       "53752  13998  这套房子有啥问题吗  我看价格不高       3            租约还有两年    666\n",
       "53753  13998  这套房子有啥问题吗  我看价格不高       4             都有学位的    666\n",
       "53754  13999             我看看时间吧       0               没有呢    666\n",
       "53755  13999             我看看时间吧       1             今天新上的    666\n",
       "53756  13999             我看看时间吧       2  房子我也没看过呢，不知道是几号楼    666\n",
       "\n",
       "[53757 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrecessForSentence(Dataset):\n",
    "    \"\"\"\n",
    "    对文本进行处理\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_tokenizer, df, input_categories,max_char_len = 103):\n",
    "        \"\"\"\n",
    "        bert_tokenizer :分词器\n",
    "        file     :语料文件\n",
    "        \"\"\"\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.max_seq_len = max_char_len\n",
    "        self.seqs, self.seq_masks, self.seq_segments, self.labels = self.get_input(df,input_categories, self.bert_tokenizer, self.max_seq_len)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx], self.seq_masks[idx], self.seq_segments[idx], self.labels[idx]\n",
    "    \n",
    "    def _convert_to_transformer_inputs(self,question, answer, tokenizer, max_sequence_length):\n",
    "        def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "            inputs = tokenizer.encode_plus(str1, str2,\n",
    "                add_special_tokens=True,\n",
    "                max_length=length,\n",
    "                truncation_strategy=truncation_strategy,\n",
    "                #truncation=True\n",
    "                )\n",
    "\n",
    "            input_ids =  inputs[\"input_ids\"]\n",
    "            input_masks = [1] * len(input_ids)\n",
    "            input_segments = inputs[\"token_type_ids\"]\n",
    "            padding_length = length - len(input_ids)\n",
    "            padding_id = tokenizer.pad_token_id\n",
    "            input_ids = input_ids + ([padding_id] * padding_length)\n",
    "            input_masks = input_masks + ([0] * padding_length)\n",
    "            input_segments = input_segments + ([0] * padding_length)\n",
    "\n",
    "            return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "        input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "            question, answer, 'longest_first', max_sequence_length)\n",
    "\n",
    "        return [input_ids_q, input_masks_q, input_segments_q]\n",
    "        \n",
    "    # 获取文本与标签\n",
    "    def get_input(self, df,columns, tokenizer, max_sequence_length,test=False):\n",
    "\n",
    "        input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "        input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "        for _, instance in tqdm(df[columns].iterrows()):\n",
    "            query,reply = instance.query, instance.reply\n",
    "\n",
    "            ids_q, masks_q, segments_q= \\\n",
    "            self._convert_to_transformer_inputs(query, reply, tokenizer, max_sequence_length)\n",
    "\n",
    "            input_ids_q.append(ids_q)\n",
    "            input_masks_q.append(masks_q)\n",
    "            input_segments_q.append(segments_q)\n",
    "            \n",
    "        labels = df['label'].values\n",
    "        return torch.Tensor(input_ids_q).type(torch.long),torch.Tensor(input_masks_q).type(torch.long),torch.Tensor(input_segments_q).type(torch.long),torch.Tensor(labels).type(torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertwwmModel(nn.Module):\n",
    "    def __init__(self,dropout=0.5,num_classes=2):\n",
    "        super(BertwwmModel,self).__init__()\n",
    "        config = BertConfig.from_pretrained('./preTrainModel/chinese-roberta-wwm-ext-large/') \n",
    "        config.output_hidden_states = False \n",
    "        self.bertwwm = BertModel.from_pretrained('./preTrainModel/chinese-roberta-wwm-ext-large/', \n",
    "                                             config=config)\n",
    "        self.dropout=dropout\n",
    "        self.device=torch.device(\"cuda\")\n",
    "        self.num_classes = num_classes\n",
    "        self.linear = nn.Linear(1024*4, num_classes)\n",
    "        for param in self.bertwwm.parameters():\n",
    "            param.requires_grad=True\n",
    "    \n",
    "    def forward(self, q_id, q_mask, q_atn):\n",
    "        q_embedding = self.bertwwm(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "        q = nn.AdaptiveAvgPool2d((1,q_embedding.shape[-1]))(q_embedding).squeeze(1)\n",
    "        a = nn.AdaptiveMaxPool2d((1,q_embedding.shape[-1]))(q_embedding).squeeze(1)\n",
    "        t = q_embedding[:,-1]\n",
    "        e = q_embedding[:, 0]\n",
    "        merged = torch.cat([q, a, t, e], dim=1)\n",
    "        x = nn.Dropout(self.dropout)(merged)\n",
    "        logits=self.linear(x)\n",
    "        probabilities =F.softmax(logits, dim=-1)\n",
    "        return logits,probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FGM:对抗性训练\n",
    "\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1e-6, emb_name='bertwwm.embeddings.'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='bertwwm.embeddings.'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name: \n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        \n",
    "    def attack_multi_emd(self, epsilon=1e-6, emd_names = ['bertwwm.embeddings.']):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                is_update = False\n",
    "                for emd_name in emd_names:\n",
    "                    if emd_name in name:\n",
    "                        is_update = True\n",
    "                        break\n",
    "                if is_update:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    norm = torch.norm(param.grad)\n",
    "                    if norm != 0 and not torch.isnan(norm):\n",
    "                        r_at = epsilon * param.grad / norm\n",
    "                        param.data.add_(r_at)\n",
    "\n",
    "    def restore_multi_emd(self, emd_names = ['bertwwm.embeddings.']):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                is_update = False\n",
    "                for emd_name in emd_names:\n",
    "                    if emd_name in name:\n",
    "                        is_update = True\n",
    "                        break\n",
    "                if is_update:\n",
    "                    assert name in self.backup\n",
    "                    param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focalloss计算损失\n",
    "#gamma 2,alpha 0.25\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_predictions(output_probabilities, targets):\n",
    "\n",
    "    _, out_classes = output_probabilities.max(dim=1)\n",
    "    correct = (out_classes == targets).sum()\n",
    "    return correct.item()\n",
    "\n",
    "\n",
    "def train(model, fgm,dataloader,optimizer, criterion,epoch_number, max_gradient_norm):\n",
    "\n",
    "    # Switch the model to train mode.\n",
    "    model.train()\n",
    "    device = model.device\n",
    "    epoch_start = time.time()\n",
    "    batch_time_avg = 0.0\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    tqdm_batch_iterator = tqdm(dataloader)\n",
    "    for batch_index, (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in enumerate(tqdm_batch_iterator):\n",
    "        batch_start = time.time()\n",
    "        # Move input and output data to the GPU if it is used.\n",
    "        seqs, masks, segments, labels = batch_seqs.to(device), batch_seq_masks.to(device), batch_seq_segments.to(device), batch_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # 正常训练\n",
    "        logits, probs  = model(seqs, masks, segments)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 对抗训练\n",
    "#         fgm.attack() # 在embedding上添加对抗扰动\n",
    "#         logits_adv, probs_adv = model(seqs, masks, segments)\n",
    "#         loss_adv = criterion(logits_adv, labels)\n",
    "#         loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "#         fgm.restore() # 恢复embedding参数\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n",
    "        optimizer.step()\n",
    "        batch_time_avg += time.time() - batch_start\n",
    "        running_loss += loss.item()\n",
    "        correct_preds += correct_predictions(probs, labels)\n",
    "        description = \"Avg. batch proc. time: {:.4f}s, loss: {:.4f}\"\\\n",
    "                      .format(batch_time_avg/(batch_index+1), running_loss/(batch_index+1))\n",
    "        tqdm_batch_iterator.set_description(description)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = correct_preds / len(dataloader.dataset)\n",
    "    return epoch_time, epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion):\n",
    "\n",
    "    # Switch to evaluate mode.\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    epoch_start = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    all_prob = []\n",
    "    all_labels = []\n",
    "    # Deactivate autograd for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in dataloader:\n",
    "            # Move input and output data to the GPU if one is used.\n",
    "            seqs = batch_seqs.to(device)\n",
    "            masks = batch_seq_masks.to(device)\n",
    "            segments = batch_seq_segments.to(device)\n",
    "            labels = batch_labels.to(device)\n",
    "            logits, probs = model(seqs, masks, segments)\n",
    "            loss = criterion(logits, labels)\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += correct_predictions(probs, labels)\n",
    "            all_prob.extend(probs[:,1].cpu().numpy())\n",
    "            all_labels.extend(batch_labels)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = running_accuracy / (len(dataloader.dataset))\n",
    "    return epoch_time, epoch_loss, epoch_accuracy, roc_auc_score(all_labels, all_prob)\n",
    "\n",
    "\n",
    "\n",
    "def test(model, dataloader):\n",
    "    # Switch the model to eval mode.\n",
    "    label_res=[]\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    time_start = time.time()\n",
    "    batch_time = 0.0\n",
    "    \n",
    "    # Deactivate autograd for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in dataloader:\n",
    "            batch_start = time.time()\n",
    "            # Move input and output data to the GPU if one is used.\n",
    "            seqs, masks, segments, labels = batch_seqs.to(device), batch_seq_masks.to(device), batch_seq_segments.to(device), batch_labels.to(device)\n",
    "            _, probabilities = model(seqs, masks, segments)\n",
    "            _, out_classes = probabilities.max(dim=1)\n",
    "#             print(out_classes)\n",
    "            label_res.extend(out_classes.cpu().numpy())\n",
    "            batch_time += time.time() - batch_start\n",
    "\n",
    "    batch_time /= len(dataloader)\n",
    "    total_time = time.time() - time_start\n",
    "#     accuracy /= (len(dataloader.dataset))\n",
    "    return batch_time, total_time,label_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:00, 1445.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Loading test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53757it [00:35, 1503.79it/s]\n"
     ]
    }
   ],
   "source": [
    "input_categories = ['query','reply']\n",
    "output_categories = 'label'\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "batch_size=16\n",
    "print(\"\\t* Loading test data...\")\n",
    "test_data = DataPrecessForSentence(bertwwm_tokenizer,df_test,input_categories,MAX_SEQUENCE_LENGTH)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17268it [00:10, 1667.52it/s]\n",
      "4317it [00:02, 1680.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model:0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Training Albert model on device: cuda,fold:0 ====================\n",
      "* Training epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 0.8041s, loss: 0.3855: 100%|██████████| 1080/1080 [14:35<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 875.8299s, loss = 0.3855, accuracy: 84.4626%\n",
      "* Validation for epoch 1:\n",
      "-> Valid. time: 69.8246s, loss: 0.3240, accuracy: 87.6303%, auc: 0.9269\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 1.0876s, loss: 0.2451: 100%|██████████| 1080/1080 [19:49<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 1189.0196s, loss = 0.2451, accuracy: 90.5027%\n",
      "* Validation for epoch 2:\n",
      "-> Valid. time: 76.1443s, loss: 0.3031, accuracy: 88.4410%, auc: 0.9371\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 1.1221s, loss: 0.1630: 100%|██████████| 1080/1080 [20:26<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 1226.8280s, loss = 0.1630, accuracy: 94.0120%\n",
      "* Validation for epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [00:00, 1175.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Valid. time: 71.6313s, loss: 0.3578, accuracy: 87.9546%, auc: 0.9353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17268it [00:54, 319.36it/s]\n",
      "4317it [00:18, 233.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model:1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Training Albert model on device: cuda,fold:1 ====================\n",
      "* Training epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 1.0475s, loss: 0.3897: 100%|██████████| 1080/1080 [19:04<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 1144.4041s, loss = 0.3897, accuracy: 84.2136%\n",
      "* Validation for epoch 1:\n",
      "-> Valid. time: 71.3436s, loss: 0.3351, accuracy: 85.6845%, auc: 0.9227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 1.1011s, loss: 0.2549: 100%|██████████| 1080/1080 [20:04<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 1204.6013s, loss = 0.2549, accuracy: 90.2536%\n",
      "* Validation for epoch 2:\n",
      "-> Valid. time: 76.6887s, loss: 0.3058, accuracy: 87.6535%, auc: 0.9301\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 1.1495s, loss: 0.1666: 100%|██████████| 1080/1080 [20:57<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 1257.7982s, loss = 0.1666, accuracy: 93.8673%\n",
      "* Validation for epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141it [00:00, 1406.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Valid. time: 71.2313s, loss: 0.3554, accuracy: 87.1207%, auc: 0.9245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17268it [00:37, 461.05it/s] \n",
      "4317it [00:06, 666.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model:2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Training Albert model on device: cuda,fold:2 ====================\n",
      "* Training epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 1.1248s, loss: 0.3734: 100%|██████████| 1080/1080 [20:30<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 1230.1680s, loss = 0.3734, accuracy: 84.7058%\n",
      "* Validation for epoch 1:\n",
      "-> Valid. time: 82.7830s, loss: 0.3157, accuracy: 87.0280%, auc: 0.9219\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 1.0851s, loss: 0.2451: 100%|██████████| 1080/1080 [19:46<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 1186.6395s, loss = 0.2451, accuracy: 90.6416%\n",
      "* Validation for epoch 2:\n",
      "-> Valid. time: 74.9429s, loss: 0.3219, accuracy: 87.1438%, auc: 0.9228\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 0.7670s, loss: 0.1611: 100%|██████████| 1080/1080 [13:55<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 835.3343s, loss = 0.1611, accuracy: 93.9136%\n",
      "* Validation for epoch 3:\n",
      "-> Valid. time: 57.4065s, loss: 0.3458, accuracy: 88.7190%, auc: 0.9267\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17268it [00:10, 1675.37it/s]\n",
      "4317it [00:02, 1687.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model:3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Training Albert model on device: cuda,fold:3 ====================\n",
      "* Training epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 0.7341s, loss: 0.4300: 100%|██████████| 1080/1080 [13:17<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 797.1097s, loss = 0.4300, accuracy: 81.9666%\n",
      "* Validation for epoch 1:\n",
      "-> Valid. time: 59.4487s, loss: 0.3050, accuracy: 88.0009%, auc: 0.9192\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 0.6946s, loss: 0.2768: 100%|██████████| 1080/1080 [12:34<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 754.9611s, loss = 0.2768, accuracy: 89.2576%\n",
      "* Validation for epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Valid. time: 55.5610s, loss: 0.3165, accuracy: 87.7924%, auc: 0.9354\n",
      "\n",
      "* Training epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 0.7029s, loss: 0.1839: 100%|██████████| 1080/1080 [12:43<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 763.6416s, loss = 0.1839, accuracy: 93.3287%\n",
      "* Validation for epoch 3:\n",
      "-> Valid. time: 58.5930s, loss: 0.3036, accuracy: 89.5298%, auc: 0.9361\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17268it [00:11, 1536.28it/s]\n",
      "4317it [00:02, 1613.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model:4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Training Albert model on device: cuda,fold:4 ====================\n",
      "* Training epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 0.7468s, loss: 0.3660: 100%|██████████| 1080/1080 [13:31<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 811.2535s, loss = 0.3660, accuracy: 84.8216%\n",
      "* Validation for epoch 1:\n",
      "-> Valid. time: 59.9028s, loss: 0.3130, accuracy: 87.4913%, auc: 0.9208\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 0.6912s, loss: 0.2341: 100%|██████████| 1080/1080 [12:30<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 750.7724s, loss = 0.2341, accuracy: 90.7980%\n",
      "* Validation for epoch 2:\n",
      "-> Valid. time: 57.7384s, loss: 0.3132, accuracy: 88.7653%, auc: 0.9299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyou/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Avg. batch proc. time: 0.7675s, loss: 0.1455: 100%|██████████| 1080/1080 [13:54<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 834.3293s, loss = 0.1455, accuracy: 94.7823%\n",
      "* Validation for epoch 3:\n",
      "-> Valid. time: 58.8226s, loss: 0.3554, accuracy: 88.5569%, auc: 0.9278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#N折交叉验证\n",
    "\n",
    "gkf = GroupKFold(n_splits=5).split(X=train_data.reply, groups=train_data.id)\n",
    "\n",
    "valid_preds = [0,0,0,0,0]\n",
    "test_preds = [0,0,0,0,0]\n",
    "\n",
    "batch_size=16\n",
    "epochs=3\n",
    "lr=2e-05\n",
    "patience=3\n",
    "max_grad_norm=10.0\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion = FocalLoss(gamma=0)\n",
    "oof = np.zeros((len(train_data),1))\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    dev_res=[]\n",
    "    test_res=[]\n",
    "    best_score = 0.0\n",
    "    start_epoch = 1\n",
    "    # Data for loss curves plot\n",
    "    epochs_count = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    #训练集\n",
    "    t_data = DataPrecessForSentence(bertwwm_tokenizer, train_data.iloc[train_idx],input_categories,MAX_SEQUENCE_LENGTH)\n",
    "    train_loader = DataLoader(t_data, shuffle=True, batch_size=batch_size)\n",
    "    #验证集\n",
    "    d_data = DataPrecessForSentence(bertwwm_tokenizer, train_data.iloc[valid_idx],input_categories,MAX_SEQUENCE_LENGTH)\n",
    "    dev_loader = DataLoader(d_data, shuffle=False, batch_size=batch_size)\n",
    "    #开始训练\n",
    "    # -------------------- Model definition ------------------- #\n",
    "    print(\"\\t* Building model:{}...\".format(fold))\n",
    "    model = BertwwmModel().to(device)\n",
    "    \n",
    "    #初始化FGM\n",
    "    fgm = FGM(model)\n",
    "    \n",
    "    # 待优化的参数\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "            {\n",
    "                    'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                    'weight_decay':0.01\n",
    "            },\n",
    "            {\n",
    "                    'params':[p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                    'weight_decay':0.0\n",
    "            }\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", \n",
    "                                                               factor=0.85, patience=0)\n",
    "    \n",
    "    print(\"\\n\", 20 * \"=\", \"Training Albert model on device: {},fold:{}\".format(device,fold), 20 * \"=\")\n",
    "    patience_counter = 0\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        epochs_count.append(epoch)\n",
    "        print(\"* Training epoch {}:\".format(epoch))\n",
    "        epoch_time, epoch_loss, epoch_accuracy = train(model,fgm, train_loader, optimizer, criterion, epoch, max_grad_norm)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(\"-> Training time: {:.4f}s, loss = {:.4f}, accuracy: {:.4f}%\"\n",
    "              .format(epoch_time, epoch_loss, (epoch_accuracy*100)))\n",
    "        print(\"* Validation for epoch {}:\".format(epoch))\n",
    "        epoch_time, epoch_loss, epoch_accuracy , epoch_auc= validate(model, dev_loader,criterion)\n",
    "        valid_losses.append(epoch_loss)\n",
    "        print(\"-> Valid. time: {:.4f}s, loss: {:.4f}, accuracy: {:.4f}%, auc: {:.4f}\\n\"\n",
    "              .format(epoch_time, epoch_loss, (epoch_accuracy*100), epoch_auc))\n",
    "        # Update the optimizer's learning rate with the scheduler.\n",
    "        scheduler.step(epoch_accuracy)\n",
    "        # Early stopping on validation accuracy.\n",
    "        if epoch_accuracy < best_score:\n",
    "            patience_counter += 1 \n",
    "        else:\n",
    "            best_score = epoch_accuracy\n",
    "            patience_counter = 0\n",
    "            \n",
    "            batch_time, total_time, dev_res= test(model, dev_loader)\n",
    "            oof[valid_idx] =[[i] for i in dev_res]\n",
    "            valid_preds[fold]=dev_res\n",
    "            batch_time, total_time, test_res=test(model, test_loader)\n",
    "            test_preds[fold]=test_res\n",
    "#             f1,t = search_f1(valid_outputs, valid_preds[-1])\n",
    "#             print('validation score = ', f1)\n",
    "        if patience_counter >= patience:\n",
    "            print(\"-> Early stopping: patience limit reached, stopping...\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def search_f1(y_true, y_pred):\n",
    "    best = 0\n",
    "    best_t = 0\n",
    "    for i in range(30,60):\n",
    "        tres = i / 100\n",
    "        y_pred_bin =  (y_pred > tres).astype(int)\n",
    "        score = f1_score(y_true, y_pred_bin)\n",
    "        if score > best:\n",
    "            best = score\n",
    "            best_t = tres\n",
    "    print('best', best)\n",
    "    print('thres', best_t)\n",
    "    return best, best_t\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best 0.7637795275590551\n",
      "thres 0.3\n"
     ]
    }
   ],
   "source": [
    "outputs = compute_output_arrays(train_data, output_categories)\n",
    "best_score, best_t = search_f1(outputs,oof)\n",
    "sub = np.average(test_preds, axis=0)\n",
    "sub = sub > best_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['label'] = sub.astype(int)\n",
    "df_test[['id','id_sub','label']].to_csv('./submission_file/submission_roberta_large_new.csv',index=False, header=None,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = np.average(test_preds, axis=0)\n",
    "# sub = sub > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['label'] = sub.astype(int)\n",
    "# df_test[['id','id_sub','label']].to_csv('./submission_file/submission_bert_wwm_focalloss_fgm.csv',index=False, header=None,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
