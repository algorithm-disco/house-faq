{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from hanziconv import HanziConv\n",
    "from transformers import *\n",
    "from transformers.optimization import AdamW\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_tokenizer = BertTokenizer.from_pretrained('./preTrainModel/albert_chinese_base/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir='./models/'\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlbertModel,self).__init__()\n",
    "        self.albert=AlbertForSequenceClassification.from_pretrained('./preTrainModel/albert_chinese_base/')\n",
    "        self.device=torch.device(\"cuda\")\n",
    "        for param in self.albert.parameters():\n",
    "            param.requires_grad=True\n",
    "    \n",
    "    def forward(self, batch_seqs, batch_seq_masks, batch_seq_segments, labels):\n",
    "        loss, logits = self.albert(input_ids = batch_seqs, attention_mask = batch_seq_masks, \n",
    "                              token_type_ids=batch_seq_segments, labels = labels)[:2]\n",
    "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        return loss, logits, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrecessForSentence(Dataset):\n",
    "    \"\"\"\n",
    "    对文本进行处理\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_tokenizer, file, max_char_len = 103):\n",
    "        \"\"\"\n",
    "        bert_tokenizer :分词器\n",
    "        file     :语料文件\n",
    "        \"\"\"\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.max_seq_len = max_char_len\n",
    "        self.seqs, self.seq_masks, self.seq_segments, self.labels = self.get_input(file)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx], self.seq_masks[idx], self.seq_segments[idx], self.labels[idx]\n",
    "        \n",
    "    # 获取文本与标签\n",
    "    def get_input(self, file,test=False):\n",
    "        \"\"\"\n",
    "        通对输入文本进行分词、ID化、截断、填充等流程得到最终的可用于模型输入的序列。\n",
    "        入参:\n",
    "            dataset     : pandas的dataframe格式，包含三列，第一,二列为文本，第三列为标签。标签取值为{0,1}，其中0表示负样本，1代表正样本。\n",
    "            max_seq_len : 目标序列长度，该值需要预先对文本长度进行分别得到，可以设置为小于等于512（BERT的最长文本序列长度为512）的整数。\n",
    "        出参:\n",
    "            seq         : 在入参seq的头尾分别拼接了'CLS'与'SEP'符号，如果长度仍小于max_seq_len，则使用0在尾部进行了填充。\n",
    "            seq_mask    : 只包含0、1且长度等于seq的序列，用于表征seq中的符号是否是有意义的，如果seq序列对应位上为填充符号，\n",
    "                          那么取值为1，否则为0。\n",
    "            seq_segment : shape等于seq，因为是单句，所以取值都为0。\n",
    "            labels      : 标签取值为{0,1}，其中0表示负样本，1代表正样本。\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(file)\n",
    "        sentences_1 = map(HanziConv.toSimplified, df['query'].values)\n",
    "        sentences_2 = map(HanziConv.toSimplified, df['reply'].values)\n",
    "        labels = df['label'].values\n",
    "        # 切词\n",
    "        tokens_seq_1 = list(map(self.bert_tokenizer.tokenize, sentences_1))\n",
    "        tokens_seq_2 = list(map(self.bert_tokenizer.tokenize, sentences_2))\n",
    "        # 获取定长序列及其mask\n",
    "        result = list(map(self.trunate_and_pad, tokens_seq_1, tokens_seq_2))\n",
    "        seqs = [i[0] for i in result]\n",
    "        seq_masks = [i[1] for i in result]\n",
    "        seq_segments = [i[2] for i in result]\n",
    "        return torch.Tensor(seqs).type(torch.long), torch.Tensor(seq_masks).type(torch.long), torch.Tensor(seq_segments).type(torch.long), torch.Tensor(labels).type(torch.long)\n",
    "    \n",
    "    def trunate_and_pad(self, tokens_seq_1, tokens_seq_2):\n",
    "        \"\"\"\n",
    "        1. 如果是单句序列，按照BERT中的序列处理方式，需要在输入序列头尾分别拼接特殊字符'CLS'与'SEP'，\n",
    "           因此不包含两个特殊字符的序列长度应该小于等于max_seq_len-2，如果序列长度大于该值需要那么进行截断。\n",
    "        2. 对输入的序列 最终形成['CLS',seq,'SEP']的序列，该序列的长度如果小于max_seq_len，那么使用0进行填充。\n",
    "        入参: \n",
    "            seq_1       : 输入序列，在本处其为单个句子。\n",
    "            seq_2       : 输入序列，在本处其为单个句子。\n",
    "            max_seq_len : 拼接'CLS'与'SEP'这两个特殊字符后的序列长度\n",
    "        \n",
    "        出参:\n",
    "            seq         : 在入参seq的头尾分别拼接了'CLS'与'SEP'符号，如果长度仍小于max_seq_len，则使用0在尾部进行了填充。\n",
    "            seq_mask    : 只包含0、1且长度等于seq的序列，用于表征seq中的符号是否是有意义的，如果seq序列对应位上为填充符号，\n",
    "                          那么取值为1，否则为0。\n",
    "            seq_segment : shape等于seq，单句，取值都为0 ，双句按照01切分\n",
    "           \n",
    "        \"\"\"\n",
    "        # 对超长序列进行截断\n",
    "        if len(tokens_seq_1) > ((self.max_seq_len - 3)//2):\n",
    "            tokens_seq_1 = tokens_seq_1[0:(self.max_seq_len - 3)//2]\n",
    "        if len(tokens_seq_2) > ((self.max_seq_len - 3)//2):\n",
    "            tokens_seq_2 = tokens_seq_2[0:(self.max_seq_len - 3)//2]\n",
    "        # 分别在首尾拼接特殊符号\n",
    "        seq = ['[CLS]'] + tokens_seq_1 + ['[SEP]'] + tokens_seq_2 + ['[SEP]']\n",
    "        seq_segment = [0] * (len(tokens_seq_1) + 2) + [1] * (len(tokens_seq_2) + 1)\n",
    "        # ID化\n",
    "        seq = self.bert_tokenizer.convert_tokens_to_ids(seq)\n",
    "        # 根据max_seq_len与seq的长度产生填充序列\n",
    "        padding = [0] * (self.max_seq_len - len(seq))\n",
    "        # 创建seq_mask\n",
    "        seq_mask = [1] * len(seq) + padding\n",
    "        # 创建seq_segment\n",
    "        seq_segment = seq_segment + padding\n",
    "        # 对seq拼接填充序列\n",
    "        seq += padding\n",
    "        assert len(seq) == self.max_seq_len\n",
    "        assert len(seq_mask) == self.max_seq_len\n",
    "        assert len(seq_segment) == self.max_seq_len\n",
    "        return seq, seq_mask, seq_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Loading training data...\n",
      "\t* Loading validation data...\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "epochs=10\n",
    "lr=2e-05\n",
    "patience=3\n",
    "max_grad_norm=10.0\n",
    "\n",
    "PATH='./'\n",
    "train_file = PATH+'train.csv'\n",
    "dev_file = PATH+'dev.csv'\n",
    "test_file=PATH+'test.csv'\n",
    "# test_file = PATH+'test.csv'\n",
    "\n",
    "print(\"\\t* Loading training data...\")\n",
    "train_data = DataPrecessForSentence(albert_tokenizer, train_file)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "print(\"\\t* Loading validation data...\")\n",
    "dev_data = DataPrecessForSentence(albert_tokenizer,dev_file)\n",
    "dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model...\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Model definition ------------------- #\n",
    "print(\"\\t* Building model...\")\n",
    "model = AlbertModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 待优化的参数\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "                'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                'weight_decay':0.01\n",
    "        },\n",
    "        {\n",
    "                'params':[p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay':0.0\n",
    "        }\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", \n",
    "                                                           factor=0.85, patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_predictions(output_probabilities, targets):\n",
    "\n",
    "    _, out_classes = output_probabilities.max(dim=1)\n",
    "    correct = (out_classes == targets).sum()\n",
    "    return correct.item()\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, epoch_number, max_gradient_norm):\n",
    "\n",
    "    # Switch the model to train mode.\n",
    "    model.train()\n",
    "    device = model.device\n",
    "    epoch_start = time.time()\n",
    "    batch_time_avg = 0.0\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    tqdm_batch_iterator = tqdm(dataloader)\n",
    "    for batch_index, (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in enumerate(tqdm_batch_iterator):\n",
    "        batch_start = time.time()\n",
    "        # Move input and output data to the GPU if it is used.\n",
    "        seqs, masks, segments, labels = batch_seqs.to(device), batch_seq_masks.to(device), batch_seq_segments.to(device), batch_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits, probabilities = model(seqs, masks, segments, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n",
    "        optimizer.step()\n",
    "        batch_time_avg += time.time() - batch_start\n",
    "        running_loss += loss.item()\n",
    "        correct_preds += correct_predictions(probabilities, labels)\n",
    "        description = \"Avg. batch proc. time: {:.4f}s, loss: {:.4f}\"\\\n",
    "                      .format(batch_time_avg/(batch_index+1), running_loss/(batch_index+1))\n",
    "        tqdm_batch_iterator.set_description(description)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = correct_preds / len(dataloader.dataset)\n",
    "    return epoch_time, epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def validate(model, dataloader):\n",
    "\n",
    "    # Switch to evaluate mode.\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    epoch_start = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    all_prob = []\n",
    "    all_labels = []\n",
    "    # Deactivate autograd for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in dataloader:\n",
    "            # Move input and output data to the GPU if one is used.\n",
    "            seqs = batch_seqs.to(device)\n",
    "            masks = batch_seq_masks.to(device)\n",
    "            segments = batch_seq_segments.to(device)\n",
    "            labels = batch_labels.to(device)\n",
    "            loss, logits, probabilities = model(seqs, masks, segments, labels)\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += correct_predictions(probabilities, labels)\n",
    "            print(probabilities.shape)\n",
    "            all_prob.extend(probabilities[:,1].cpu().numpy())\n",
    "            all_labels.extend(batch_labels)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = running_accuracy / (len(dataloader.dataset))\n",
    "    return epoch_time, epoch_loss, epoch_accuracy, roc_auc_score(all_labels, all_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([16, 2])\n",
      "\t* Validation loss before training: 0.8605, accuracy: 27.4500%, auc: 0.5036\n"
     ]
    }
   ],
   "source": [
    "best_score = 0.0\n",
    "start_epoch = 1\n",
    "# Data for loss curves plot\n",
    "epochs_count = []\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "_, valid_loss, valid_accuracy, auc = validate(model, dev_loader)\n",
    "print(\"\\t* Validation loss before training: {:.4f}, accuracy: {:.4f}%, auc: {:.4f}\".format(valid_loss, (valid_accuracy*100), auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Training epochs ------------------- #\n",
    "print(\"\\n\", 20 * \"=\", \"Training Albert model on device: {}\".format(device), 20 * \"=\")\n",
    "patience_counter = 0\n",
    "for epoch in range(start_epoch, epochs + 1):\n",
    "    epochs_count.append(epoch)\n",
    "    print(\"* Training epoch {}:\".format(epoch))\n",
    "    epoch_time, epoch_loss, epoch_accuracy = train(model, train_loader, optimizer, epoch, max_grad_norm)\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(\"-> Training time: {:.4f}s, loss = {:.4f}, accuracy: {:.4f}%\"\n",
    "          .format(epoch_time, epoch_loss, (epoch_accuracy*100)))\n",
    "    print(\"* Validation for epoch {}:\".format(epoch))\n",
    "    epoch_time, epoch_loss, epoch_accuracy , epoch_auc= validate(model, dev_loader)\n",
    "    valid_losses.append(epoch_loss)\n",
    "    print(\"-> Valid. time: {:.4f}s, loss: {:.4f}, accuracy: {:.4f}%, auc: {:.4f}\\n\"\n",
    "          .format(epoch_time, epoch_loss, (epoch_accuracy*100), epoch_auc))\n",
    "    # Update the optimizer's learning rate with the scheduler.\n",
    "    scheduler.step(epoch_accuracy)\n",
    "    # Early stopping on validation accuracy.\n",
    "    if epoch_accuracy < best_score:\n",
    "        patience_counter += 1\n",
    "    else:\n",
    "        best_score = epoch_accuracy\n",
    "        patience_counter = 0\n",
    "        torch.save({\"epoch\": epoch, \n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"best_score\": best_score,\n",
    "                    \"epochs_count\": epochs_count,\n",
    "                    \"train_losses\": train_losses,\n",
    "                    \"valid_losses\": valid_losses},\n",
    "                    os.path.join(target_dir, \"pytorch_albert_best.pth.tar\"))\n",
    "    if patience_counter >= patience:\n",
    "        print(\"-> Early stopping: patience limit reached, stopping...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_res=[]\n",
    "def test(model, dataloader):\n",
    "    # Switch the model to eval mode.\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    time_start = time.time()\n",
    "    batch_time = 0.0\n",
    "    \n",
    "    # Deactivate autograd for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in dataloader:\n",
    "            batch_start = time.time()\n",
    "            # Move input and output data to the GPU if one is used.\n",
    "            seqs, masks, segments, labels = batch_seqs.to(device), batch_seq_masks.to(device), batch_seq_segments.to(device), batch_labels.to(device)\n",
    "            _, _, probabilities = model(seqs, masks, segments, labels)\n",
    "            _, out_classes = probabilities.max(dim=1)\n",
    "#             print(out_classes)\n",
    "            label_res.extend(out_classes.cpu().numpy())\n",
    "            batch_time += time.time() - batch_start\n",
    "\n",
    "    batch_time /= len(dataloader)\n",
    "    total_time = time.time() - time_start\n",
    "#     accuracy /= (len(dataloader.dataset))\n",
    "    return batch_time, total_time,label_res\n",
    "\n",
    "class AlbertModelTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlbertModelTest, self).__init__()\n",
    "        config = AlbertConfig.from_pretrained('./preTrainModel/albert_chinese_base/')\n",
    "        self.albert = AlbertForSequenceClassification(config)  # /bert_pretrain/\n",
    "        self.device = torch.device(\"cuda\")\n",
    "\n",
    "    def forward(self, batch_seqs, batch_seq_masks, batch_seq_segments, labels):\n",
    "        loss, logits = self.albert(input_ids = batch_seqs, attention_mask = batch_seq_masks, \n",
    "                              token_type_ids=batch_seq_segments, labels = labels)[:2]\n",
    "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        return loss, logits, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "pretrained_file='./models/pytorch_albert_best.pth.tar'\n",
    "\n",
    "\n",
    "print(\"\\t* Loading test data...\")\n",
    "test_data = DataPrecessForSentence(albert_tokenizer,test_file)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# albert_tokenizer = BertTokenizer.from_pretrained(\"voidful/albert_chinese_base\",cache_dir='./preTrainModel/albert_chinese_base')\n",
    "print(20 * \"=\", \" Preparing for testing \", 20 * \"=\")\n",
    "# if platform == \"linux\" or platform == \"linux2\":\n",
    "checkpoint = torch.load(pretrained_file)\n",
    "# else:\n",
    "#     checkpoint = torch.load(pretrained_file, map_location=device)\n",
    "\n",
    "print(\"\\t* Building model...\")\n",
    "model = AlbertModelTest().to(device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "print(20 * \"=\", \" Testing Albert model on device: {} \".format(device), 20 * \"=\")\n",
    "batch_time, total_time, label_res= test(model, test_loader)\n",
    "print(\"\\n-> Average batch processing time: {:.4f}s, total test time: {:.4f}s\\n\".\n",
    "      format(batch_time, total_time))\n",
    "print(label_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(label_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission=pd.read_csv('./sample_submission.tsv',sep='\\t',header=None)\n",
    "sample_submission.columns=['id','idx','label_UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['label']=label_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.drop('label_UNK',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('./submission_file/sample_submission.tsv',index=False,sep='\\t', encoding='utf-8',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}