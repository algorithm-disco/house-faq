{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "# from transformers import *\n",
    "from transformers.optimization import AdamW\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albertsmall_tokenizer = BertTokenizer.from_pretrained('./preTrainModel/albert_chinese_small/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir='./models/'\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrecessForSentence(Dataset):\n",
    "    \"\"\"\n",
    "    对文本进行处理\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_tokenizer, df, input_categories,max_char_len = 103):\n",
    "        \"\"\"\n",
    "        bert_tokenizer :分词器\n",
    "        file     :语料文件\n",
    "        \"\"\"\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.max_seq_len = max_char_len\n",
    "        self.seqs, self.seq_masks, self.seq_segments, self.labels = self.get_input(df,input_categories, self.bert_tokenizer, self.max_seq_len)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx], self.seq_masks[idx], self.seq_segments[idx], self.labels[idx]\n",
    "    \n",
    "    def _convert_to_transformer_inputs(self,question, answer, tokenizer, max_sequence_length):\n",
    "        def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "            inputs = tokenizer.encode_plus(str1, str2,\n",
    "                add_special_tokens=True,\n",
    "                max_length=length,\n",
    "                truncation_strategy=truncation_strategy,\n",
    "                #truncation=True\n",
    "                )\n",
    "\n",
    "            input_ids =  inputs[\"input_ids\"]\n",
    "            input_masks = [1] * len(input_ids)\n",
    "            input_segments = inputs[\"token_type_ids\"]\n",
    "            padding_length = length - len(input_ids)\n",
    "            padding_id = tokenizer.pad_token_id\n",
    "            input_ids = input_ids + ([padding_id] * padding_length)\n",
    "            input_masks = input_masks + ([0] * padding_length)\n",
    "            input_segments = input_segments + ([0] * padding_length)\n",
    "\n",
    "            return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "        input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "            question, answer, 'longest_first', max_sequence_length)\n",
    "\n",
    "        return [input_ids_q, input_masks_q, input_segments_q]\n",
    "        \n",
    "    # 获取文本与标签\n",
    "    def get_input(self, df,columns, tokenizer, max_sequence_length,test=False):\n",
    "\n",
    "        input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "        input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "        for _, instance in tqdm(df[columns].iterrows()):\n",
    "            query,reply = instance.query, instance.reply\n",
    "\n",
    "            ids_q, masks_q, segments_q= \\\n",
    "            self._convert_to_transformer_inputs(query, reply, tokenizer, max_sequence_length)\n",
    "\n",
    "            input_ids_q.append(ids_q)\n",
    "            input_masks_q.append(masks_q)\n",
    "            input_segments_q.append(segments_q)\n",
    "            \n",
    "        labels = df['label'].values\n",
    "        return torch.Tensor(input_ids_q).type(torch.long),torch.Tensor(input_masks_q).type(torch.long),torch.Tensor(input_segments_q).type(torch.long),torch.Tensor(labels).type(torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:00, 1022.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18200it [00:27, 669.83it/s] \n",
      "141it [00:00, 1406.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Loading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:01, 1066.42it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size=16\n",
    "epochs=10\n",
    "lr=2e-05\n",
    "patience=3\n",
    "max_grad_norm=10.0\n",
    "\n",
    "PATH='./'\n",
    "train_file = PATH+'train.csv'\n",
    "dev_file = PATH+'dev.csv'\n",
    "test_file=PATH+'test.csv'\n",
    "# test_file = PATH+'test.csv'\n",
    "\n",
    "input_categories = ['query','reply']\n",
    "output_categories = 'label'\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "batch_size=32\n",
    "\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_dev = pd.read_csv(dev_file)\n",
    "\n",
    "print(\"\\t* Loading training data...\")\n",
    "train_data = DataPrecessForSentence(albertsmall_tokenizer,df_train,input_categories,MAX_SEQUENCE_LENGTH)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "print(\"\\t* Loading validation data...\")\n",
    "dev_data = DataPrecessForSentence(albertsmall_tokenizer,df_dev,input_categories,MAX_SEQUENCE_LENGTH)\n",
    "dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.seqs, self.seq_masks, self.seq_segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 待优化的参数\n",
    "# param_optimizer = list(model.named_parameters())\n",
    "# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "# optimizer_grouped_parameters = [\n",
    "#         {\n",
    "#                 'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "#                 'weight_decay':0.01\n",
    "#         },\n",
    "#         {\n",
    "#                 'params':[p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "#                 'weight_decay':0.0\n",
    "#         }\n",
    "# ]\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", \n",
    "#                                                            factor=0.85, patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_predictions(output_probabilities, targets):\n",
    "\n",
    "    _, out_classes = output_probabilities.max(dim=1)\n",
    "    correct = (out_classes == targets).sum()\n",
    "    return correct.item()\n",
    "\n",
    "\n",
    "# def train(model, dataloader,optimizer, criterion,epoch_number, max_gradient_norm):\n",
    "\n",
    "#     # Switch the model to train mode.\n",
    "#     model.train()\n",
    "# #     device = model.device\n",
    "#     epoch_start = time.time()\n",
    "#     batch_time_avg = 0.0\n",
    "#     running_loss = 0.0\n",
    "#     correct_preds = 0\n",
    "#     tqdm_batch_iterator = tqdm(dataloader)\n",
    "#     for batch_index, (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in enumerate(tqdm_batch_iterator):\n",
    "#         batch_start = time.time()\n",
    "#         # Move input and output data to the GPU if it is used.\n",
    "#         seqs, masks, segments, labels = batch_seqs.cuda(), batch_seq_masks.cuda(), batch_seq_segments.cuda(), batch_labels.cuda()\n",
    "#         optimizer.zero_grad()\n",
    "#         logits, probs  = model(seqs, masks, segments)\n",
    "#         loss = criterion(logits, labels)\n",
    "#         loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n",
    "#         optimizer.step()\n",
    "#         batch_time_avg += time.time() - batch_start\n",
    "#         running_loss += loss.item()\n",
    "#         correct_preds += correct_predictions(probs, labels)\n",
    "#         description = \"Avg. batch proc. time: {:.4f}s, loss: {:.4f}\"\\\n",
    "#                       .format(batch_time_avg/(batch_index+1), running_loss/(batch_index+1))\n",
    "#         tqdm_batch_iterator.set_description(description)\n",
    "#     epoch_time = time.time() - epoch_start\n",
    "#     epoch_loss = running_loss / len(dataloader)\n",
    "#     epoch_accuracy = correct_preds / len(dataloader.dataset)\n",
    "#     return epoch_time, epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion):\n",
    "\n",
    "    # Switch to evaluate mode.\n",
    "    model.eval()\n",
    "#     device = model.device\n",
    "    epoch_start = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    all_prob = []\n",
    "    all_labels = []\n",
    "    # Deactivate autograd for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in dataloader:\n",
    "            # Move input and output data to the GPU if one is used.\n",
    "            seqs = batch_seqs.cuda()\n",
    "            masks = batch_seq_masks.cuda()\n",
    "            segments = batch_seq_segments.cuda()\n",
    "            labels = batch_labels.cuda()\n",
    "            logits, probs = model(seqs, masks, segments)\n",
    "            loss = criterion(logits, labels)\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += correct_predictions(probs, labels)\n",
    "            all_prob.extend(probs[:,1].cpu().numpy())\n",
    "            all_labels.extend(batch_labels)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = running_accuracy / (len(dataloader.dataset))\n",
    "    return epoch_time, epoch_loss, epoch_accuracy, roc_auc_score(all_labels, all_prob)\n",
    "\n",
    "\n",
    "\n",
    "def test(model, dataloader):\n",
    "    # Switch the model to eval mode.\n",
    "    label_res=[]\n",
    "    model.eval()\n",
    "#     device = model.device\n",
    "    time_start = time.time()\n",
    "    batch_time = 0.0\n",
    "    \n",
    "    # Deactivate autograd for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in dataloader:\n",
    "            batch_start = time.time()\n",
    "            # Move input and output data to the GPU if one is used.\n",
    "            seqs, masks, segments, labels = batch_seqs.cuda(), batch_seq_masks.cuda(), batch_seq_segments.cuda(), batch_labels.cuda()\n",
    "            _, probabilities = model(seqs, masks, segments)\n",
    "            _, out_classes = probabilities.max(dim=1)\n",
    "#             print(out_classes)\n",
    "            label_res.extend(out_classes.cpu().numpy())\n",
    "            batch_time += time.time() - batch_start\n",
    "\n",
    "    batch_time /= len(dataloader)\n",
    "    total_time = time.time() - time_start\n",
    "#     accuracy /= (len(dataloader.dataset))\n",
    "    return batch_time, total_time,label_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model...\n"
     ]
    }
   ],
   "source": [
    "class AlbertSmallModel(nn.Module):\n",
    "    def __init__(self,dropout=0.5,num_classes=2):\n",
    "        super(AlbertSmallModel,self).__init__()\n",
    "        config = BertConfig.from_pretrained('./preTrainModel/albert_chinese_small/') \n",
    "        config.output_hidden_states = True\n",
    "        self.AlbertSmall = BertModel.from_pretrained('./preTrainModel/albert_chinese_small/', \n",
    "                                             config=config)\n",
    "        self.hidden_size = 384\n",
    "        self.dropout=dropout\n",
    "        self.device=torch.device(\"cuda\")\n",
    "        self.num_classes = num_classes\n",
    "        self.linear = nn.Linear(10*self.hidden_size, num_classes)\n",
    "        self.projection = nn.Sequential(nn.Linear(4*self.hidden_size, self.hidden_size), \n",
    "                                        nn.ReLU())\n",
    "        self.classification = nn.Sequential(nn.Linear(4*4*self.hidden_size, self.hidden_size),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=self.dropout),\n",
    "                                            nn.Linear(self.hidden_size, self.hidden_size//2),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=self.dropout),\n",
    "                                            nn.Linear(self.hidden_size//2, self.num_classes))  \n",
    "        \n",
    "        for param in self.AlbertSmall.parameters():\n",
    "            param.requires_grad=True\n",
    "    \n",
    "    def forward(self, q_id, q_mask, q_atn):\n",
    "\n",
    "        mask1=q_mask.to(torch.float32)-q_atn.to(torch.float32) #batch_size*seq_max\n",
    "        mask2=q_atn.to(torch.float32)   #batch_size*seq_max\n",
    "\n",
    "        q_embedding,pooler_output,hidden_states = self.AlbertSmall(q_id, attention_mask=q_mask, token_type_ids=q_atn)\n",
    "#         print(len(hidden_states))\n",
    "#         print(hidden_states[-1]) \n",
    "#         print(q_embedding)\n",
    "        #试试倒数二三层的hidden加入\n",
    "        hidden_feature=hidden_states[-2]+hidden_states[-3]\n",
    "        hidden_avg = nn.AdaptiveAvgPool2d((1,hidden_feature.shape[-1]))(hidden_feature).squeeze(1)\n",
    "        hidden_max = nn.AdaptiveMaxPool2d((1,hidden_feature.shape[-1]))(hidden_feature).squeeze(1)\n",
    "#         mask1=mask1.unsqueeze(-1) #batch_size*seq_max*1\n",
    "        q1=q_embedding*mask1.unsqueeze(-1)  #batch_size*seq_max*dim\n",
    "#         mask2=mask2.unsqueeze(-1) #batch_size*seq_max*1\n",
    "        q2=q_embedding*mask2.unsqueeze(-1)  #batch_size*seq_max*dim\n",
    "        \n",
    "        #进行attention部分\n",
    "        mask_attn1=mask1.unsqueeze(-1)  #batch_size*seqlen1*1\n",
    "        mask_attn2=mask2.unsqueeze(1)   #batch_size*1*seqlen2\n",
    "        mask_similarity_matrix=torch.bmm(mask_attn1,mask_attn2)  #batch_size_seqlen1*seqlen2\n",
    "        mask_similarity_matrix=(mask_similarity_matrix-1.)*10000  #batch_size_seqlen1*seqlen2\n",
    "        similarity_matrix=torch.bmm(q1,q2.permute(0,2,1))   #batch_size*seqlen1*seqlen2\n",
    "        similarity_matrix=similarity_matrix+mask_similarity_matrix  #batch_size*seqlen1*seqlen2\n",
    "        similarity_matrix_transpose=similarity_matrix.permute(0,2,1)   #batch_size*seqlen2*seqlen1\n",
    "        \n",
    "        alpha1=F.softmax(similarity_matrix_transpose,dim=-1)  #batch_size*seqlen2*seqlen1\n",
    "        alpha2=F.softmax(similarity_matrix,dim=-1)    #batch_size*seqlen1*seqlen2\n",
    "        \n",
    "        q1_tilde=torch.bmm(alpha2,q2)    #batch_size*seqlen1*dim\n",
    "        q2_tilde=torch.bmm(alpha1,q1)    #batch_size*seqlen2*dim\n",
    "        \n",
    "         #进行composition部分\n",
    "        q1_combined=torch.cat([q1,q1_tilde,torch.abs(q1-q1_tilde),torch.mul(q1,q1_tilde)],dim=-1)  #batch_size*seqlen1*4dim\n",
    "        q2_combined=torch.cat([q2,q2_tilde,torch.abs(q2-q2_tilde),torch.mul(q2,q2_tilde)],dim=-1)   #batch_size*seqlen2*4dim\n",
    "        \n",
    "        # 映射一下\n",
    "        projected_q1 = self.projection(q1_combined)  #batch_size*seqlen1*dim\n",
    "        projected_q2 = self.projection(q2_combined)  #batch_size*seqlen1*dim\n",
    "        \n",
    "        def reduce_mean_with_mask(q, mask):\n",
    "            dim=q.shape[-1]  #dim\n",
    "            seq_len=torch.sum(mask,1).unsqueeze(1)  #batch_size*1\n",
    "            seq_len_tiled=seq_len.repeat(1,dim)  #batch_size*dim\n",
    "            q_sum=torch.sum(q,1)  #batch_size*dim\n",
    "            return q_sum/seq_len_tiled\n",
    "        \n",
    "        # 平均池化 + 最大池化\n",
    "        q1_avg=reduce_mean_with_mask(projected_q1, mask1)   #batch_size*4dim\n",
    "        q1_max=torch.max(projected_q1,1)[0]   #batch_size*4dim\n",
    "        q2_avg =reduce_mean_with_mask(projected_q2, mask2)  #batch_size*4dim\n",
    "        q2_max=torch.max(projected_q2,1)[0]   #batch_size*4dim\n",
    "        \n",
    "        # 拼接成最后的特征向量\n",
    "        q = nn.AdaptiveAvgPool2d((1,q_embedding.shape[-1]))(q_embedding).squeeze(1)\n",
    "        a = nn.AdaptiveMaxPool2d((1,q_embedding.shape[-1]))(q_embedding).squeeze(1)\n",
    "        t = q_embedding[:,-1]\n",
    "        e = q_embedding[:, 0] \n",
    "        \n",
    "        \n",
    "        merged = torch.cat([q,a,t,e,hidden_avg,hidden_max,q1_avg, q1_max, q2_avg, q2_max], dim=1)  #batch_size*16dim\n",
    "        #分类\n",
    "#         logits = self.classification(merged.float())\n",
    "        \n",
    "        \n",
    "#         q = nn.AdaptiveAvgPool2d((1,q_embedding.shape[-1]))(q_embedding).squeeze(1)\n",
    "#         a = nn.AdaptiveMaxPool2d((1,q_embedding.shape[-1]))(q_embedding).squeeze(1)\n",
    "#         t = q_embedding[:,-1]\n",
    "#         e = q_embedding[:, 0]\n",
    "#         merged = torch.cat([q, a, t, e], dim=1)\n",
    "        x = nn.Dropout(self.dropout)(merged)\n",
    "        logits=self.linear(x)\n",
    "        probabilities =F.softmax(logits, dim=-1)\n",
    "        return logits,probabilities\n",
    "    \n",
    "    \n",
    "    \n",
    "# -------------------- Model definition ------------------- #\n",
    "print(\"\\t* Building model...\")\n",
    "model = AlbertSmallModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Validation loss before training: 0.7635, accuracy: 72.9000%, auc: 0.4969\n"
     ]
    }
   ],
   "source": [
    "best_score = 0.0\n",
    "start_epoch = 1\n",
    "# Data for loss curves plot\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs_count = []\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "_, valid_loss, valid_accuracy, auc = validate(model, dev_loader, criterion)\n",
    "print(\"\\t* Validation loss before training: {:.4f}, accuracy: {:.4f}%, auc: {:.4f}\".format(valid_loss, (valid_accuracy*100), auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([-1, -2,  3])\n",
    "b = torch.Tensor([5, -2,  33]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.,  4., 99.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9669, -0.0629, -1.3784],\n",
       "        [ 0.4956, -0.6945, -1.9827],\n",
       "        [-0.3600, -1.0455,  0.3489]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9669, -0.0629,  0.3489])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a,0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([[[1,2],[3,4],[3,4]],[[8,9],[88,12],[3,7]],[[18,29],[838,122],[663,4]],[[10,26],[8368,12552],[53,4]]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  3,   4],\n",
       "        [  3,   7],\n",
       "        [663,   4],\n",
       "        [ 53,   4]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2],\n",
       "        [ 8,  9],\n",
       "        [18, 29],\n",
       "        [10, 26]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}